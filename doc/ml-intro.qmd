---
title: "Basic Machine Learning with R: Some Limnological Toy Examples"
author: "Thomas Petzoldt"
format:
  html:
    toc: true
    number-sections: true
    page-layout: article
  pdf:
    toc: true
    number-sections: true
    shift-heading-level-by: -1
editor: source
bibliography: ann.bib
csl: apa
always_allow_html: true
self-contained: true
---

```{r echo=FALSE, include=FALSE}
library("knitr")
library("DiagrammeR")
library("nnet")
library("dplyr")
library("ggplot2")
library("mlr3")
library("mlr3learners")
library("mlr3filters")
library("lubridate")
```

## Introduction

The following examples aim to give an idea about analysing water
quality data sets with machine learning (ML) techniques. The examples
are selected to start as easy as possible and can of course not give
a comprehensive overview over contemporary modelling techniques.

Problems solved with ML techniques can be sub-divided in different cateogries,
supervised and non-supervised, classification and regression. The examples below
concentrate exclusively on supervised regression problems. Examples for other
problems can be found in textbooks like @Lantz2019 or @Rhys2020 and in many 
online resources.

We start with a technical example and then apply the methodology to a
practical data sets.

## Methods

### Neural networks

In the following, let's start with one of the early packages for
neural networks in **R** [@RCore2022], the package **nnet** from
@Ripley1996 and @Venables2002. Later we can switch to other packages and algorithms.

The idea behind artificial neural networks (ANNs) is an analogy to
biological neural networks. Neural networks consist of many simple and
similar building blocks, called neurons (@fig-neuron) because of their
analogies to their biological counterpart.

<!---
```{r echo=FALSE, fig.height=3.5}
#| label: fig-neuron
#| fig-cap: "Neuron with 3 inputs (x1, x2, x3), weights (w1, w2, w3) and one output (y)."
grViz("digraph neuron {
  rankdir=LR
  splines=line
 
  {
    node [style=solid, shape=circle, label='', width=.5]
    n
  }
  node [shape=none, margin=0.05, width=0.0];
    x1 x2 x3 y
  
  x1 -> n [label=<w<SUB>1</SUB>>]
  x2 -> n [label=<w<SUB>2</SUB>>]
  x3 -> n [label=<w<SUB>3</SUB>>]
  n  -> y [label=<w<SUB>4</SUB>>]
}")
```
--->

![Neuron with 3 inputs (x1, x2, x3), weights (w1, w2, w3) and one output (y).](img/diagrammer-neuron.png){#fig-neuron width=40%}

A neuron has several inputs $x_i$ with individual weighting factors
$w_i$. The weighted inputs are then combined with a weighted sum
and then passed on to the output $y$ via a transfer function.

The transfer function can have different structure, where in the classical case,
a sigmoidal function was one of the most common:

$$
f(\mathbf{x}, \mathbf{w}) = \frac{1}{1 + e^{-\sum_{i=1}^n x_i \cdot w_i}}
$$

The sigmoidal function can look very different, depending on which weights
and x-values are used (@fig-sigmoid). The picture can become even more
flexible, if several sigmoidal functions are added. This way, a neural
network consisting of a sufficient number of neurons can be fit to any
multidimensional and nonlinear data. The neurons can be connected in
different topologies, e.g. a three layer feed-forward network
(@fig-forward-network).

```{r, echo=FALSE,fig.height=8, fig.width=8}
#| label: fig-sigmoid
#| fig-cap: "Flexibility of a sigmoidal transfer function. A single neuron can 
#| exhibit increasing and decreasing sigmoidal pattern, step functions, near-exponential 
#| increase and decrease, saturation or linear shapes."


sigmoid <- function(x) {
  ifelse(x < -15, 0, ifelse(x > 15, 1, 1.0 / (1.0 + exp(-x))))
}

par(mfrow=c(3,3), las=1)
x <- seq(-5, 5, length.out=100)
plot(x, sigmoid(x), type="l")
plot(x, sigmoid(-x), type="l")
x <- seq(-5, 5, length.out=100)
plot(x, sigmoid(10*x), type="l")
plot(x, sigmoid(-10*x), type="l")
x <- seq(-5, 0, length.out=100)
plot(x, sigmoid(x), type="l")
x <- seq(0, 5, length.out=100)
plot(x, sigmoid(-2*x), type="l")
plot(x, sigmoid(2*x), type="l")
x <- seq(-2.5, 2.5, length.out=100)
plot(x, sigmoid(x/5), type="l")
plot(x, sigmoid(-x/5), type="l")
```

<!---
```{r, echo=FALSE, fig.height=3.5}
#| label: fig-forward-network
#| fig-cap: "Fully connected feed-forward network with three layers, for example
#| and 3 neurons in the input  layer, 5 in the hidden layer and 2 in the output
#| layer.  Deep neural networks have more than one hidden layer."

grViz("digraph ANN {
  
  rankdir=LR
  splines=line
  
  node [fixedsize=true, label='']
  
  subgraph cluster_0 {
    color=white
    node [style=solid, shape=circle]
    x1 x2 x3
    label = 'input layer'
  }
  
  subgraph cluster_1 {
    color=white
    node [style=solid,shape=circle]
    a12 a22 a32 a42 a52
    label = 'hidden layer'
  }
  
  subgraph cluster_2 {
    color=white
    node [style=solid, shape=circle]
    o1 o2
    label='output layer'
  }
  
  {x1 x2 x3} -> {a12 a22 a32 a42 a52} -> {o1 o2}
  
}")
```

**Note**: In contrast to the classical sigmoidal function, modern deep neural 
network frameworks like [Keras](https://keras.io/) prefer other transfer functions, 
e.g. a linear ramp function (ReLU, rectified linear unit). 
The ReLU is linear and surprisingly much simpler than the sigmoidal, but has 
several advantages. It needs less computation, can simplified to "zero" 
(allows sparse activation) and stabilizes training. Nonlinearity can then be 
introduced by additional layers. A short explanation and an overview of 
some relevant papers can be found in 
[Wikipedia](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)).

--->
![Fully connected feed-forward network with three layers, for example and 3 neurons in the input  layer, 5 in the hidden layer and 2 in the output.  Deep neural networks have more than one hidden layer.](img/diagrammer-ann.png){#fig-forward-network width=60%}


### A toy example

Let's assume a simple toy data set with a single variabble $y$
depending on $x$ following an optimum function with some noise
(@fig-toydata). We can see clearly that the ANN with 5 hidden neurons
can fit the data well, while one neuron was obviously not enough. But
if we look closely, we see also, that the latter ANN shows some sign
of overfitting at the beginning.

```{r}
#| label: fig-toydata
#| fig-cap: "Toy data set."
# Generate some test data
set.seed(123)
x <- seq(0, 100, 1)
y <- 100 * dlnorm(x, 4, .7) + rnorm(x, sd=0.05)

plot(x, y)
```

Now, we transform the $y$ variable to the interval \[0, 1\] and fit a
neural network with 1 or 5 hidden neurons. The `trace` argument is set
to `FALSE` to suppress intermediate results in the script. For
interactive use, I would recommended to set `trace=TRUE`.

```{r}
#| label: fig-fitted-ann
#| fig-cap: "Toy data set scaled to [0,1] and two fitted neural networks with 1 hidden neuron (blue) and with 5 hidden neurons (red)."
library(nnet)

set.seed(3142)

# Transform y-data (y must be between 0 and 1)
y <- (y - min(y))/(max(y) - min(y))

# Plot the transformed data
plot(x, y)

# Fit the neural net
nn1 <- nnet(x, y, size=1, trace = FALSE)
nn2 <- nnet(x, y, size=5, trace = FALSE, maxit=500)

lines(x, predict(nn1), col="blue")
lines(x, predict(nn2), col="red")
```

## Case study 1: Dependence of phytoplankton growth rate on temperature and light

### Data set

The data set was digitized from figures of @Dauta1990, who analyzed growth rate 
dependency of four algae species on light intensity at different temperatures. Now, 
we aim to create a regression model to predict growth rate ($\mu$) at any 
values of light and temperature within the measured range for each species. 
To save space, we demonstrate the procedure for only two of the four species.

The data can also be fitted with parametric models instead of ML techniques.
This is explained in another tutorial.

First we load and plot the data set (@fig-dauta-dataset).

```{r}
#| label: fig-dauta-dataset
#| fig-cap: "Growthrate (1/d) dependent on temperature (°C) and light (μmol·m−2·s−1)."

library("dplyr")
library("ggplot2")
dauta <- read.csv("dauta2.csv")
ggplot(data=dauta, aes(light, growthrate)) + geom_point() + 
  facet_grid(species ~ temperature)
```
For a first run, we use only the two species with the best data quality and scale
the data to the interval \[0, 1\]:

```{r}
dauta <- subset(dauta, species %in% c("Chlorella", "Fragilaria"))
ymin  <- min(dauta$growthrate)
ymax  <- max(dauta$growthrate)
y     <- (dauta$growthrate - ymin)/(ymax - ymin)
```

Then we encode the species from character to a numeric variable and assign the
predictors (species, temperature and light) to a separate variable `x`.

```{r}
dauta$species_i <- as.numeric(factor(dauta$species))
x <- dauta[, c("species_i", "temperature", "light")]
```

### Fit of a neural network

Now we can fit a neural network. In the code below, we do this several times in 
a `for`-loop and select the best fitting network. This is a quite basic method, 
but it can lead to overfitting.

```{r}
set.seed(1423)
hidden <- 10    # number of hidden neurons
maxit  <- 1000  # maximum number of iterations per training replicate
rep    <- 5     # number of training replicates
value <- Inf
for (i in 1:rep) {
  net <- nnet(x, y, size=hidden, maxit=maxit, trace=FALSE)
  if (net$value < value) {
    n1 <- net
    value <- net$value
  }
}
```

Now, we can compare the fitted values with the original data (@fig-dauta-pred-obs). 

```{r}
#| label: fig-dauta-pred-obs
#| fig-cap: "Comparison between predicted and observed values of the Dauta data set."
plot(y, n1$fitted.values, pch = "+", 
     col = dauta$species_i, xlab = "observed", ylab = "predicted")
cat("R^2=", 1 - var(residuals(n1))/var(y), "\n") # coefficient of determination
```

### Visualization

There are of course many different ways to visualize the results. First we may 
compare the model with individual data sets (@fig-dauta-ann-single).

```{r}
#| label: fig-dauta-ann-single
#| fig-cap: "Comparison between observed (points) and predicted growth rates for species = 1 at 25°C."


## Test of neural net with a single data set
sdat <- subset(dauta, species_i == 1 & temperature == 25)

## set a series of values for the x axis
light  <- seq(0, 700, 5)
yy   <- predict(n1, data.frame(species_i = 1, temperature = 25, light = light))

## retransform growth rate from [0, 1] to original scale
growthrate   <- yy * (ymax - ymin) + ymin

plot(sdat$light, sdat$growthrate)
lines(light, growthrate, col = "red", lwd = 2)
```
We can also plot the outcome for all species and temperatures together (@fig-dauta-ann-multi). 
Here the function `expand.grid` creates a data frame with all combinations of the 
explanation variables `species_i`, `light` and `temperature`. This data frame can 
then be used in `predict` where the variable names of the new data must exactly match 
the variable names in the original data set used for fitting the model.

```{r}
#| label: fig-dauta-ann-multi
#| fig-cap: "Comparison between observed (points) and predicted growth rates for two species."

newdata <- expand.grid(
  species_i = unique(dauta$species_i),
  temperature = unique(dauta$temperature),
  light = seq(0, 700, 5)
)

yy <- predict(n1, newdata)[, 1]

## retransform predicted values to original scale
newdata$growthrate <- yy * (ymax - ymin) + ymin

## assign species names corresponding to the species number
species <- levels(factor(dauta$species))
newdata$species <- species[newdata$species_i]

ggplot(data=dauta, mapping=aes(x=light, y=growthrate)) +
  geom_point() +
  geom_line(data=newdata, mapping=aes(x=light, y=growthrate), color="red", linewidth=1) +
  facet_grid(species ~ temperature)
```

### Exercises

1. Modify the example above and test the procedure with a different number of 
   neurons. Modify also the value in `set.seed()`. You may experience, that
   not all trials will work equally well. Sometimes you will see that models
   show additional wiggles, a clear sign of overfitting.
2. Now try to fit the whole data set with all four species. Find good
   settings by trial and error that produce both a high coefficient of
   determination and plausible figures.

## Case Study 2: Relationship between environmental variables and phytoplankton in a lake

The example is taken from early experiments of @Kielb1994,
@Petzoldt1996a and @Petzoldt1999 using parts of the long-term dataset from the Saidenbach
Reservoir. The selection of input variables was rather explorative at
that time, but should be ok for demonstration purposes.

The question was, if it is possible to reconstruct the amount of
phytoplankton from environmental data. Here we have to note, that this
is a pattern recognition example and no forecast, because
phytoplankton and environmental data were measured at the same time.

### The data set

For the simulation experiments, measured data of the Saidenbach dam
were used (cf. @Horn2006). As the data were not always exactly
equidistant, an interpolation was carried out with cubic splines to an
interval of 14d, with subsequent manual post-correction to avoid
wiggles.

The variables shown in the table, except Date and XBM were used as
inputs for a neural network, while phytoplankton biomass (XBM) in the
epilimnion was used as output. The DATE variable will be used to
select rows. The phytoplankton biomass XBM is then square root
transformed and then normalized to the interval \[0, 1\]. Square root
transformation aims to downweight large peaks while normalizing is
necessary because the standard learning algorithm requires this.

```{r, echo=FALSE}
#| label: tbl-variables
#| tbl-cap: Variables in the plankton data set.
vars <- read.csv("variables.csv")
knitr::kable(vars)
```

### Application of a feed-forward neural network

#### Data Management
We start with some data management:

- convert the DATE column to a date format

- optionally, apply a square root transformation to downweight the
  influence of extreme values and to increase resolution for lower
  values.

- rescale the $y$ variable to fit in the interval \[0, 1\]. In the
  example below, some additional "room" was added, so that minimum and
  maximum does not exactly match zero and one. This makes the
  procedure more robust, but must be taken into account for the
  backtransformation.

- split the data matrix to separate the target variable $y$ and the
  explanation variables $x$

```{r}
library(nnet)

## load datas set and convert DATE to date format
dat <- read.csv("phytoplankton-cqc.csv")
dat$DATE <- as.Date(dat$DATE)

## optional: transform dependent variable,
## because it contains very extreme values
dat$XBE <- dat$XBE^0.5

## rescale dependent variable to interval [0, 1]
dat$XBE <- 0.1 + dat$XBE/(1.2 * max(dat$XBE))


## split into target and explanation variables
select <- c("DEPTH", "ZMIX", "ST", "TE", "TH",
           "PO4_PE", "PO4_PH", "NO3E", "NO3H",
           "SIE", "SIH", "O2_SATE", "O2_SATH")

y <- dat$XBE
x <- dat[select]
```

#### Modell fitting

Now we can fit a first neural network. Because some networks did not
converge with the default settings, some of the learning parameters
(`decay` and `maxit`) were adapted by trial and error.

Note also the `set.seed()` function that starts the random number with a fixed
value (the "seed") to ensure reproducibility of the examples in this document.
It is a good idea to play with it in order to see how different the obtained 
results can be.

```{r}
#| label: fig-fitted-plankton
#| fig-cap: "Measured phytoplankton data (circles, transformed scale) and fitted neural network (red line)."

set.seed(123)

n_wts <- 15
nn <- nnet(x, y, size = n_wts, decay = 1e-3, maxit = 500, trace=FALSE)
err <- nn$value

yhat <- predict(nn)[,1]

plot(dat$DATE, dat$XBE, xlab="Date", ylab="Phytoplankton (transformed axis)")
lines(dat$DATE, yhat, col="red")
```

We can then also compute model quality criteria (cf. @Jachner2007) or
plot predicted versus measured (@fig-measured-fitted-plankton).

```{r}
#| label: fig-measured-fitted-plankton
#| fig-cap: "Neural network predictions versus measured phytoplankton data. The dotted line shows the 1:1 ratio between predicted and observed"

plot(dat$XBE, yhat, xlab="observed", ylab="predicted")
abline(a=0, b=1, col="grey", lty="dotted")
```

Sometimes, it makes sense to run more than one trial and then to
select the best. Note however that this can result in overfitting. We
will learn later, how this can be avoided.

```{r, eval=FALSE}
for (i in 1:10) {
  ## fit another candidate network nn_try
  nn_try <- nnet(x, y, size=n_wts, decay=1e-3, abstol=1e-6, trace = FALSE, maxit=500)

  ## and store it if better
  if (nn_try$value < err) {
    err <- nn_try$value
    nn <- nn_try
    #cat(err, "\n")
  }
}

yhat <- predict(nn)[,1]

plot(dat$DATE, dat$XBE, xlim="Date", ylim="Phytoplankton (transformed axis)")
lines(dat$DATE, yhat, col="red")
```

We can see that the network approximates the original data quite
well. But as we used the full data set for training, we cannot check
if the network can successfully applied to new, unkown data
sets. Therefore, it is common, to split the data set at least into two
data sets, where one is used for training and one for model
validation.

Often, the data set is split in three subsets "trainig", "validation"
and "test", where the first two are used in an automatic procedure to
avoid overfitting and the 3rd "test" data set is used for an
independent test of the model.

#### Exercises

1. Split the phytoplankton data set in two subsets, where years with
   equal number are in the training and years with unequal numbers are
   in the validation data set.  Then fit a suitable ANN and evaluate
   goodnes of fit for the training data set and the validation data
   set.

2. Repeat the same procedure with opposite assignment to training and
   validation data.  Compare the results.
   
### Model fitting with mlr3

In the last years, a large number of machine learning techniques and of related 
**R** packages were developed, each with its own advantages and philosophy.
Mastering all these and selecting the best one can be a big challenge.
On the other hand, so-called "frameworks" or "meta packages" appeared, trying to
integrate existing technologies in a unified application programming interface (API).
These packages differ again with respect to the included features, syntax and 
philosophy, so it can again be difficult where to start.

One of the most popular packages in **R** is still the **caret** (Classification 
and Regression Training) package (@caret). However in the following, let's explore 
some possibilities of the **mlr3** package (@mlr3). This has surely a personal 
component, because I had the chance to meet the authors in an enthusiastic workshop,
and after exploring the features of **mlr3** once again, I think it is a good decision 
due to its vast flexibility.

The design of the package follows a clear structure, the included algorithms are 
very flexible and configurable, it comes with extensive documentation and examples
and is relatively easy to start with. The syntax is somewhat special due to the 
employed **R6** class system, but this in turn offers high flexibility and helps
to ensure a clean and organized workspace.

#### Set up of a ML task

In a first step, we load the packages, the ML framework **mlr3**, a package with
optional learners **ml3learners** and a package for date and time computation.

Then we load the data set and may do some data managament. First, we transform the
target variable `XBM` to a reasonable scale as above and prepare 4 separate vectors
`date`, `all` (all years), `train` (uneven years) and `test` (all even years) 
that we will need later.

Then we create a task, that contains our data set and names the target variable. 
To exclude `DATE` and `XBE` from the list of explanation variables, we can use
`select`. This is one of the above mentioned object oriented **R6** functions.
In effect, the `task`-object modifies itself without the need of an assignment 
with `<-` or `=`.

```{r}
library("mlr3")
library("mlr3learners")
library("lubridate")

dat <- read.csv("phytoplankton-cqc.csv")
dat$XBE <- sqrt(dat$XBE/max(dat$XBE))

date <- as.Date(dat$DATE)

all  <- 1:nrow(dat)
train <- which(year(date) %% 2 == 0) # uneven years
test  <- which(year(date) %% 2 == 1) # even years

task <- as_task_regr(dat, target="XBE")

## select all columns except DATE and XBE
task$select(c("DEPTH", "ZMIX", "ST", "TE", "TH", "PO4_PE", 
  "PO4_PH", "NO3E", "NO3H", "SIE", "SIH", "O2_SATE", "O2_SATH"))
```

Now we can select a learner out of a huge number of available ML algorithms. 
The list of available learners can be shown with.

```{r, eval=FALSE}
mlr_learners
```

The list can be even bigger, depending on which packages were actually loaded. The learner 
`regr.nnet` is the one from the package **nnet** that we have used for the experiments above.
For our first experiment with **mlr3**, let's use random forest algorithm (@Breiman2001), 
a very powerful and robust ML algorithm, with a fast implementation in the 
package **ranger** [@Wright2017]. 

If not yet done, install the package **ranger** first:

```{r, eval=FALSE}
install.packages("ranger")
```

Now we set up a learner. Here it is possible to provide additional options,
like in the `nnet` examples above. The line is shown below and can be uncommented
if you want. The random forest learner is more robust and works with the default 
settings.

```{r}
learner = lrn("regr.ranger")
#learner = lrn("regr.nnet", maxit=1500, decay=1e-3)
```



Now comes the essential step to train the learner. In contrast to the `nnet` 
example above we use only the training data subset and compare it later to the
test data set, not used for training.
You may wonder that the function returns very quickly. It does indeed
highly CPU intensive computations, but the data set is small,
our computers are fast and the "ranger" package is very efficient. We set a 
random seed to be able to reproduce the outcome at a later time.
 
```{r}
set.seed(123)
learner$train(task, row_ids = train)
```

Now we can evaluate the results numerically:

```{r}
print(learner$model)
```

The values of RMSE and  R squared can be misleading, because they compare only with the training data. 
To get them for the test data, we first do a prediction for the test data subset and then compute the scores.
Function `msr` defines a measure. The full dictionary of measures is available via:

```{r, eval=FALSE}
mlr_measures
```


```{r}
pred <- learner$predict(task, row_ids = test)
pred$score(list(msr("regr.mse"),  
                msr("regr.rmse"),
                msr("regr.rsq")))
```

The value of $r^2$ may be considered surprisingly small, so let's look at it graphically (@fig-mlr-plankton).

To plot the results, we can make a prediction for all data, and then separately 
for the training (green) and the test data (red).

```{r}
#| label: fig-mlr-plankton
#| fig-cap: "Measured phytoplankton data (dots, transformed scale) and fitted neural network for training (green) and test data (red)."

pred_all <- learner$predict(task, row_ids = all)

plot(date, pred_all$truth, pch=16, cex=0.7, 
     col="navy", ylab="sqrt(XBM/max(XBM)")
lines(date, pred_all$response)
lines(date[test], pred_all$response[test], type="h", col="red")
lines(date[train], pred_all$response[train], type="h", col="green")
```

We see that the general pattern is well matched by the model for both, the training 
and the test data, while the short-term behaviour differs for the test case, that's why
the small $r^2$. However, we should not forget that this is a toy example with 
quite naive predictors. To improve this, we would need to consider short-term forcing.

#### Variable importance

Originally, we put all variables in that we had, but an important question of ML 
is, which of the predictor variables have the best predictive power and are really 
needed. This problem is called "variable importance" or "feature importance"
and then "feature selection", that is available in package **mlr3filters**. 
Random forest has a nice property, to have feature importance embedded.

 
```{r}
library("mlr3filters")
set.seed(123)
learner = lrn("regr.ranger", importance="impurity")
filter <- flt("importance", learner = learner)
filter$calculate(task)
as.data.table(filter)
```

While variable importance with `flt("importance", ...)` is available for random 
forest models, it cannot be applied to all learners. A list of available
filters is found at https://mlr3filters.mlr-org.com/

We see that oxygen saturation (`O2_SATE`) has highest predictive power, 
followed by Secchi depth (`ST`) and epilimnion (suface layer) temperature `TE`.
This is not surprising, as oxygen and temperature have a clear seasonal component
and `ST` is inversely related to plankton. The selection of variables looks
rather naive, but even in @Kielb1994 and @Petzoldt1996a, this was just one of the first
steps before testing suitability of neural networks for forecasting. Forecasting
was not yet very successful at that time due to lack of high frequeny data. Such data are 
now available, so it is time to make a new start.

#### Exercise

1. Repeat the example with other learnes, e.g. neural networks (`regr.nnet`), 
  k nearest neighbours (`regr.kknn`) or linear models (`regr.lm`). A description 
  of the learners can be found in the [mlr3 reference](https://mlr3learners.mlr-org.com/reference/#regression-learners)
  web page.
2. Repeat the example with a subset of predictive variables, identified by
  feature importance selection.
3. Do a recherche about feature importance methods and apply it to a neural
  network learner.

## Outlook

The examples above should give a short introduction, so that the big world of ML
techniques can be explored step by step. The upcoming mlr3 book [@mlr3book]
covers (or will cover) additional topics like classification problems, resampling 
and benchmarking, hyperparameter optimization and model interpretation.


## References

