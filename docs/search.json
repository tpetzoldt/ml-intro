[
  {
    "objectID": "ml-intro.html",
    "href": "ml-intro.html",
    "title": "Basic Machine Learning with R with Toy Examples from Aquatic Ecology",
    "section": "",
    "text": "The following examples aim to give an idea about analysing water quality data sets with machine learning (ML) techniques. The examples are selected to start as easy as possible and can of course not give a comprehensive overview over contemporary modelling techniques.\nProblems solved with ML techniques can be sub-divided in different cateogries, supervised and non-supervised, classification and regression. The examples below concentrate exclusively on supervised regression problems. Examples for other problems can be found in textbooks like Lantz (2019) or Rhys (2020) and in many online resources.\nWe start with a technical example and then apply the methodology to a practical data sets."
  },
  {
    "objectID": "ml-intro.html#methods",
    "href": "ml-intro.html#methods",
    "title": "Basic Machine Learning with R with Toy Examples from Aquatic Ecology",
    "section": "2 Methods",
    "text": "2 Methods\n\n2.1 Neural networks\nIn the following, let’s start with one of the early packages for neural networks in R (R Core Team, 2022), the package nnet from Ripley (1996) and Venables & Ripley (2002). Later we can switch to other packages and algorithms.\nThe idea behind artificial neural networks (ANNs) is an analogy to biological neural networks. Neural networks consist of many simple and similar building blocks, called neurons (Figure 1) because of their analogies to their biological counterpart.\n\n\n\n\nFigure 1: Neuron with 3 inputs (x1, x2, x3), weights (w1, w2, w3) and one output (y).\n\n\nA neuron has several inputs \\(x_i\\) with individual weighting factors \\(w_i\\). The weighted inputs are then combined with a weighted sum and then passed on to the output \\(y\\) via a transfer function.\nThe transfer function can have different structure, where in the classical case, a sigmoidal function was one of the most common:\n\\[\nf(\\mathbf{x}, \\mathbf{w}) = \\frac{1}{1 + e^{-\\sum_{i=1}^n x_i \\cdot w_i}}\n\\]\nThe sigmoidal function can look very different, depending on which weights and x-values are used (Figure 2). The picture can become even more flexible, if several sigmoidal functions are added. This way, a neural network consisting of a sufficient number of neurons can be fit to any multidimensional and nonlinear data. The neurons can be connected in different topologies, e.g. a three layer feed-forward network (Figure 3).\n\n\n\n\n\nFigure 2: Flexibility of a sigmoidal transfer function. A single neuron can exhibit increasing and decreasing sigmoidal pattern, step functions, near-exponential increase and decrease, saturation or linear shapes.\n\n\n\n\n\nNote: In contrast to the classical sigmoidal function, modern deep neural network frameworks like Keras prefer other transfer functions, e.g. a linear ramp function (ReLU, rectified linear unit). The ReLU is linear and surprisingly much simpler than the sigmoidal, but has several advantages. It needs less computation, can simplified to “zero” (allows sparse activation) and stabilizes training. Nonlinearity can then be introduced by additional layers. A short explanation and an overview of some relevant papers can be found in Wikipedia.\n\n\n\nFigure 3: Fully connected feed-forward network with three layers, for example and 3 neurons in the input layer, 5 in the hidden layer and 2 in the output. Deep neural networks have more than one hidden layer.\n\n\n\n\n2.2 A toy example\nLet’s assume a simple toy data set with a single variabble \\(y\\) depending on \\(x\\) following an optimum function with some noise (Figure 4). We can see clearly that the ANN with 5 hidden neurons can fit the data well, while one neuron was obviously not enough. But if we look closely, we see also, that the latter ANN shows some sign of overfitting at the beginning.\n\n# Generate some test data\nset.seed(123)\nx <- seq(0, 100, 1)\ny <- 100 * dlnorm(x, 4, .7) + rnorm(x, sd=0.05)\n\nplot(x, y)\n\n\n\n\nFigure 4: Toy data set.\n\n\n\n\nNow, we transform the \\(y\\) variable to the interval [0, 1] and fit a neural network with 1 or 5 hidden neurons. The trace argument is set to FALSE to suppress intermediate results in the script. For interactive use, I would recommended to set trace=TRUE.\n\nlibrary(nnet)\n\nset.seed(3142)\n\n# Transform y-data (y must be between 0 and 1)\ny <- (y - min(y))/(max(y) - min(y))\n\n# Plot the transformed data\nplot(x, y)\n\n# Fit the neural net\nnn1 <- nnet(x, y, size=1, trace = FALSE)\nnn2 <- nnet(x, y, size=5, trace = FALSE, maxit=500)\n\nlines(x, predict(nn1), col=\"blue\")\nlines(x, predict(nn2), col=\"red\")\n\n\n\n\nFigure 5: Toy data set scaled to [0,1] and two fitted neural networks with 1 hidden neuron (blue) and with 5 hidden neurons (red)."
  },
  {
    "objectID": "ml-intro.html#case-study-1-dependence-of-phytoplankton-growth-rate-on-temperature-and-light",
    "href": "ml-intro.html#case-study-1-dependence-of-phytoplankton-growth-rate-on-temperature-and-light",
    "title": "Basic Machine Learning with R with Toy Examples from Aquatic Ecology",
    "section": "3 Case study 1: Dependence of phytoplankton growth rate on temperature and light",
    "text": "3 Case study 1: Dependence of phytoplankton growth rate on temperature and light\n\n3.1 Data set\nThe data set was digitized from figures of Dauta et al. (1990), who analyzed growth rate dependency of four algae species on light intensity at different temperatures. Now, we aim to create a regression model to predict growth rate (\\(\\mu\\)) at any values of light and temperature within the measured range for each species. To save space, we demonstrate the procedure for only two of the four species.\nThe data can also be fitted with parametric models instead of ML techniques. This is explained in another tutorial.\nFirst we load and plot the data set (Figure 6).\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\ndauta <- read.csv(\"data/dauta2.csv\")\nggplot(data=dauta, aes(light, growthrate)) + geom_point() + \n  facet_grid(species ~ temperature)\n\n\n\n\nFigure 6: Growthrate (1/d) dependent on temperature (°C) and light (μmol·m−2·s−1).\n\n\n\n\nFor a first run, we use only the two species with the best data quality and scale the data to the interval [0, 1]:\n\ndauta <- subset(dauta, species %in% c(\"Chlorella\", \"Fragilaria\"))\nymin  <- min(dauta$growthrate)\nymax  <- max(dauta$growthrate)\ny     <- (dauta$growthrate - ymin)/(ymax - ymin)\n\nThen we encode the species from character to a numeric variable and assign the predictors (species, temperature and light) to a separate variable x.\n\ndauta$species_i <- as.numeric(factor(dauta$species))\nx <- dauta[, c(\"species_i\", \"temperature\", \"light\")]\n\n\n\n3.2 Fit of a neural network\nNow we can fit a neural network. In the code below, we do this several times in a for-loop and select the best fitting network. This is a quite basic method, but it can lead to overfitting.\n\nset.seed(1423)\nhidden <- 10    # number of hidden neurons\nmaxit  <- 1000  # maximum number of iterations per training replicate\nrep    <- 5     # number of training replicates\nvalue <- Inf\nfor (i in 1:rep) {\n  net <- nnet(x, y, size=hidden, maxit=maxit, trace=FALSE)\n  if (net$value < value) {\n    n1 <- net\n    value <- net$value\n  }\n}\n\nNow, we can compare the fitted values with the original data (Figure 7).\n\nplot(y, n1$fitted.values, pch = \"+\", \n     col = dauta$species_i, xlab = \"observed\", ylab = \"predicted\")\ncat(\"R^2=\", 1 - var(residuals(n1))/var(y), \"\\n\") # coefficient of determination\n\nR^2= 0.9802464 \n\n\n\n\n\nFigure 7: Comparison between predicted and observed values of the Dauta data set.\n\n\n\n\n\n\n3.3 Visualization\nThere are of course many different ways to visualize the results. First we may compare the model with individual data sets (Figure 8).\n\n## Test of neural net with a single data set\nsdat <- subset(dauta, species_i == 1 & temperature == 25)\n\n## set a series of values for the x axis\nlight  <- seq(0, 700, 5)\nyy   <- predict(n1, data.frame(species_i = 1, temperature = 25, light = light))\n\n## retransform growth rate from [0, 1] to original scale\ngrowthrate   <- yy * (ymax - ymin) + ymin\n\nplot(sdat$light, sdat$growthrate)\nlines(light, growthrate, col = \"red\", lwd = 2)\n\n\n\n\nFigure 8: Comparison between observed (points) and predicted growth rates for species = 1 at 25°C.\n\n\n\n\nWe can also plot the outcome for all species and temperatures together (Figure 9). Here the function expand.grid creates a data frame with all combinations of the explanation variables species_i, light and temperature. This data frame can then be used in predict where the variable names of the new data must exactly match the variable names in the original data set used for fitting the model.\n\nnewdata <- expand.grid(\n  species_i = unique(dauta$species_i),\n  temperature = unique(dauta$temperature),\n  light = seq(0, 700, 5)\n)\n\nyy <- predict(n1, newdata)[, 1]\n\n## retransform predicted values to original scale\nnewdata$growthrate <- yy * (ymax - ymin) + ymin\n\n## assign species names corresponding to the species number\nspecies <- levels(factor(dauta$species))\nnewdata$species <- species[newdata$species_i]\n\nggplot(data=dauta, mapping=aes(x=light, y=growthrate)) +\n  geom_point() +\n  geom_line(data=newdata, mapping=aes(x=light, y=growthrate), color=\"red\", linewidth=1) +\n  facet_grid(species ~ temperature)\n\n\n\n\nFigure 9: Comparison between observed (points) and predicted growth rates for two species.\n\n\n\n\n\n\n3.4 Exercises\n\nModify the example above and test the procedure with a different number of neurons. Modify also the value in set.seed(). You may experience, that not all trials will work equally well. Sometimes you will see that models show additional wiggles, a clear sign of overfitting.\nNow try to fit the whole data set with all four species. Find good settings by trial and error that produce both a high coefficient of determination and plausible figures."
  },
  {
    "objectID": "ml-intro.html#case-study-2-relationship-between-environmental-variables-and-phytoplankton-in-a-lake",
    "href": "ml-intro.html#case-study-2-relationship-between-environmental-variables-and-phytoplankton-in-a-lake",
    "title": "Basic Machine Learning with R with Toy Examples from Aquatic Ecology",
    "section": "4 Case Study 2: Relationship between environmental variables and phytoplankton in a lake",
    "text": "4 Case Study 2: Relationship between environmental variables and phytoplankton in a lake\nThe example is taken from early experiments of Kielb (1996), Petzoldt (1996) and Petzoldt & Benndorf (1999) using parts of the long-term dataset from the Saidenbach Reservoir. The selection of input variables was rather explorative at that time, but should be ok for demonstration purposes.\nThe question was, if it is possible to reconstruct the amount of phytoplankton from environmental data. Here we have to note, that this is a pattern recognition example and no forecast, because phytoplankton and environmental data were measured at the same time.\n\n4.1 The data set\nFor the simulation experiments, measured data of the Saidenbach dam were used (cf. Horn et al. (2006)). As the data were not always exactly equidistant, an interpolation was carried out with cubic splines to an interval of 14d, with subsequent manual post-correction to avoid wiggles.\nThe variables shown in the table, except Date and XBM were used as inputs for a neural network, while phytoplankton biomass (XBM) in the epilimnion was used as output. The DATE variable will be used to select rows. The phytoplankton biomass XBM is then square root transformed and then normalized to the interval [0, 1]. Square root transformation aims to downweight large peaks while normalizing is necessary because the standard learning algorithm requires this.\n\n\n\n\nTable 1: Variables in the plankton data set.\n\n\nNo\nVariable\nAbbreviation\n\n\n\n\n1\ndate\nDATE\n\n\n2\nphytoplankton in epilimnion\nXBE\n\n\n3\nlake depth\nDEPTH\n\n\n4\nmixing depth\nZMIX\n\n\n5\nsecchi depth\nST\n\n\n6\ntemperature in epilimnion\nTE\n\n\n7\ntemperatur in hypolimnion\nTH\n\n\n8\nconc. of PO4-P in Epilimnion\nPO4_PE\n\n\n9\nconc. of PO4-P in hypolimnion\nPO4_PH\n\n\n10\nconc of NO3-N in epilimnion\nNO3_E\n\n\n11\nconc of NO3-N in hypolimnion\nNO3_H\n\n\n12\nconc of SiO2 in epilimnion\nSIE\n\n\n13\nconc of SiO2 in hypolimnion\nSIH\n\n\n14\nsaturation of O2 in epilimnion\nO2_SATE\n\n\n15\nsaturation of O2 in hypolimnion\nO2_SATH\n\n\n\n\n\n\n\n\n4.2 Application of a feed-forward neural network\n\n4.2.1 Data Management\nWe start with some data management:\n\nconvert the DATE column to a date format\noptionally, apply a square root transformation to downweight the influence of extreme values and to increase resolution for lower values.\nrescale the \\(y\\) variable to fit in the interval [0, 1]. In the example below, some additional “room” was added, so that minimum and maximum does not exactly match zero and one. This makes the procedure more robust, but must be taken into account for the backtransformation.\nsplit the data matrix to separate the target variable \\(y\\) and the explanation variables \\(x\\)\n\n\nlibrary(nnet)\n\n## load datas set and convert DATE to date format\ndat <- read.csv(\"data/phytoplankton-cqc.csv\")\ndat$DATE <- as.Date(dat$DATE)\n\n## optional: transform dependent variable,\n## because it contains very extreme values\ndat$XBE <- dat$XBE^0.5\n\n## rescale dependent variable to interval [0, 1]\ndat$XBE <- 0.1 + dat$XBE/(1.2 * max(dat$XBE))\n\n\n## split into target and explanation variables\nselect <- c(\"DEPTH\", \"ZMIX\", \"ST\", \"TE\", \"TH\",\n           \"PO4_PE\", \"PO4_PH\", \"NO3E\", \"NO3H\",\n           \"SIE\", \"SIH\", \"O2_SATE\", \"O2_SATH\")\n\ny <- dat$XBE\nx <- dat[select]\n\n\n\n4.2.2 Modell fitting\nNow we can fit a first neural network. Because some networks did not converge with the default settings, some of the learning parameters (decay and maxit) were adapted by trial and error.\nNote also the set.seed() function that starts the random number with a fixed value (the “seed”) to ensure reproducibility of the examples in this document. It is a good idea to play with it in order to see how different the obtained results can be.\n\nset.seed(123)\n\nn_wts <- 15\nnn <- nnet(x, y, size = n_wts, decay = 1e-3, maxit = 500, trace=FALSE)\nerr <- nn$value\n\nyhat <- predict(nn)[,1]\n\nplot(dat$DATE, dat$XBE, xlab=\"Date\", ylab=\"Phytoplankton (transformed axis)\")\nlines(dat$DATE, yhat, col=\"red\")\n\n\n\n\nFigure 10: Measured phytoplankton data (circles, transformed scale) and fitted neural network (red line).\n\n\n\n\nWe can then also compute model quality criteria (cf. Jachner et al. (2007)) or plot predicted versus measured (Figure 11).\n\nplot(dat$XBE, yhat, xlab=\"observed\", ylab=\"predicted\")\nabline(a=0, b=1, col=\"grey\", lty=\"dotted\")\n\n\n\n\nFigure 11: Neural network predictions versus measured phytoplankton data. The dotted line shows the 1:1 ratio between predicted and observed\n\n\n\n\nSometimes, it makes sense to run more than one trial and then to select the best. Note however that this can result in overfitting. We will learn later, how this can be avoided.\n\nfor (i in 1:10) {\n  ## fit another candidate network nn_try\n  nn_try <- nnet(x, y, size=n_wts, decay=1e-3, abstol=1e-6, trace = FALSE, maxit=500)\n\n  ## and store it if better\n  if (nn_try$value < err) {\n    err <- nn_try$value\n    nn <- nn_try\n    #cat(err, \"\\n\")\n  }\n}\n\nyhat <- predict(nn)[,1]\n\nplot(dat$DATE, dat$XBE, xlim=\"Date\", ylim=\"Phytoplankton (transformed axis)\")\nlines(dat$DATE, yhat, col=\"red\")\n\nWe can see that the network approximates the original data quite well. But as we used the full data set for training, we cannot check if the network can successfully applied to new, unkown data sets. Therefore, it is common, to split the data set at least into two data sets, where one is used for training and one for model validation.\nOften, the data set is split in three subsets “trainig”, “validation” and “test”, where the first two are used in an automatic procedure to avoid overfitting and the 3rd “test” data set is used for an independent test of the model.\n\n\n4.2.3 Exercises\n\nSplit the phytoplankton data set in two subsets, where years with equal number are in the training and years with unequal numbers are in the validation data set. Then fit a suitable ANN and evaluate goodnes of fit for the training data set and the validation data set.\nRepeat the same procedure with opposite assignment to training and validation data. Compare the results.\n\n\n\n\n4.3 Model fitting with mlr3\nIn the last years, a large number of machine learning techniques and of related R packages were developed, each with its own advantages and philosophy. Mastering all these and selecting the best one can be a big challenge. On the other hand, so-called “frameworks” or “meta packages” appeared, trying to integrate existing technologies in a unified application programming interface (API). These packages differ again with respect to the included features, syntax and philosophy, so it can again be difficult where to start.\nOne of the most popular packages in R is still the caret (Classification and Regression Training) package (Kuhn, 2022). However in the following, let’s explore some possibilities of the mlr3 package (Lang et al., 2019). This has surely a personal component, because I had the chance to meet the authors in an enthusiastic workshop, and after exploring the features of mlr3 once again, I think it is a good decision due to its vast flexibility.\nThe design of the package follows a clear structure, the included algorithms are very flexible and configurable, it comes with extensive documentation and examples and is relatively easy to start with. The syntax is somewhat special due to the employed R6 class system, but this in turn offers high flexibility and helps to ensure a clean and organized workspace.\n\n4.3.1 Set up of a ML task\nIn a first step, we load the packages, the ML framework mlr3, a package with optional learners ml3learners and a package for date and time computation.\nThen we load the data set and may do some data managament. First, we transform the target variable XBM to a reasonable scale as above and prepare 4 separate vectors date, all (all years), train (uneven years) and test (all even years) that we will need later.\nThen we create a task, that contains our data set and names the target variable. To exclude DATE and XBE from the list of explanation variables, we can use select. This is one of the above mentioned object oriented R6 functions. In effect, the task-object modifies itself without the need of an assignment with <- or =.\n\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nlibrary(\"lubridate\")\n\ndat <- read.csv(\"data/phytoplankton-cqc.csv\")\ndat$XBE <- sqrt(dat$XBE/max(dat$XBE))\n\ndate <- as.Date(dat$DATE)\n\nall  <- 1:nrow(dat)\ntrain <- which(year(date) %% 2 == 0) # uneven years\ntest  <- which(year(date) %% 2 == 1) # even years\n\ntask <- as_task_regr(dat, target=\"XBE\")\n\n## select all columns except DATE and XBE\ntask$select(c(\"DEPTH\", \"ZMIX\", \"ST\", \"TE\", \"TH\", \"PO4_PE\", \n  \"PO4_PH\", \"NO3E\", \"NO3H\", \"SIE\", \"SIH\", \"O2_SATE\", \"O2_SATH\"))\n\nNow we can select a learner out of a huge number of available ML algorithms. The list of available learners can be shown with.\n\nmlr_learners\n\nThe list can be even bigger, depending on which packages were actually loaded. The learner regr.nnet is the one from the package nnet that we have used for the experiments above. For our first experiment with mlr3, let’s use random forest algorithm (Breiman, 2001), a very powerful and robust ML algorithm, with a fast implementation in the package ranger (Wright & Ziegler, 2017).\nIf not yet done, install the package ranger first:\n\ninstall.packages(\"ranger\")\n\nNow we set up a learner. Here it is possible to provide additional options, like in the nnet examples above. The line is shown below and can be uncommented if you want. The random forest learner is more robust and works with the default settings.\n\nlearner = lrn(\"regr.ranger\")\n#learner = lrn(\"regr.nnet\", maxit=1500, decay=1e-3)\n\nNow comes the essential step to train the learner. In contrast to the nnet example above we use only the training data subset and compare it later to the test data set, not used for training. You may wonder that the function returns very quickly. It does indeed highly CPU intensive computations, but the data set is small, our computers are fast and the “ranger” package is very efficient. We set a random seed to be able to reproduce the outcome at a later time.\n\nset.seed(123)\nlearner$train(task, row_ids = train)\n\nNow we can evaluate the results numerically:\n\nprint(learner$model)\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      104 \nNumber of independent variables:  13 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.008254066 \nR squared (OOB):                  0.3383215 \n\n\nThe values of RMSE and R squared can be misleading, because they compare only with the training data. To get them for the test data, we first do a prediction for the test data subset and then compute the scores. Function msr defines a measure. The full dictionary of measures is available via:\n\nmlr_measures\n\n\npred <- learner$predict(task, row_ids = test)\npred$score(list(msr(\"regr.mse\"),  \n                msr(\"regr.rmse\"),\n                msr(\"regr.rsq\")))\n\n   regr.mse   regr.rmse    regr.rsq \n0.002552549 0.050522761 0.150215184 \n\n\nThe value of \\(r^2\\) may be considered surprisingly small, so let’s look at it graphically (Figure 12).\nTo plot the results, we can make a prediction for all data, and then separately for the training (green) and the test data (red).\n\npred_all <- learner$predict(task, row_ids = all)\n\nplot(date, pred_all$truth, pch=16, cex=0.7, \n     col=\"navy\", ylab=\"sqrt(XBM/max(XBM)\")\nlines(date, pred_all$response)\nlines(date[test], pred_all$response[test], type=\"h\", col=\"red\")\nlines(date[train], pred_all$response[train], type=\"h\", col=\"green\")\n\n\n\n\nFigure 12: Measured phytoplankton data (dots, transformed scale) and fitted neural network for training (green) and test data (red).\n\n\n\n\nWe see that the general pattern is well matched by the model for both, the training and the test data, while the short-term behaviour differs for the test case, that’s why the small \\(r^2\\). However, we should not forget that this is a toy example with quite naive predictors. To improve this, we would need to consider short-term forcing.\n\n\n4.3.2 Variable importance\nOriginally, we put all variables in that we had, but an important question of ML is, which of the predictor variables have the best predictive power and are really needed. This problem is called “variable importance” or “feature importance” and then “feature selection”, that is available in package mlr3filters. Random forest has a nice property, to have feature importance embedded.\n\nlibrary(\"mlr3filters\")\nset.seed(123)\nlearner = lrn(\"regr.ranger\", importance=\"impurity\")\nfilter <- flt(\"importance\", learner = learner)\nfilter$calculate(task)\nas.data.table(filter)\n\n    feature      score\n 1: O2_SATE 0.40940171\n 2:      ST 0.22722108\n 3:      TE 0.14492339\n 4:    NO3H 0.14232261\n 5:  PO4_PE 0.12669678\n 6:      TH 0.07740165\n 7:  PO4_PH 0.06880431\n 8:    ZMIX 0.06510928\n 9:    NO3E 0.06425026\n10:   DEPTH 0.05720722\n11: O2_SATH 0.05637095\n12:     SIE 0.05052818\n13:     SIH 0.03427231\n\n\nWhile variable importance with flt(\"importance\", ...) is available for random forest models, it cannot be applied to all learners. A list of available filters is found at https://mlr3filters.mlr-org.com/\nWe see that oxygen saturation (O2_SATE) has highest predictive power, followed by Secchi depth (ST) and epilimnion (suface layer) temperature TE. This is not surprising, as oxygen and temperature have a clear seasonal component and ST is inversely related to plankton. The selection of variables looks rather naive, but even in Kielb (1996) and Petzoldt (1996), this was just one of the first steps before testing suitability of neural networks for forecasting. Forecasting was not yet very successful at that time due to lack of high frequeny data. Such data are now available, so it is time to make a new start.\n\n\n4.3.3 Exercise\n\nRepeat the example with other learnes, e.g. neural networks (regr.nnet), k nearest neighbours (regr.kknn) or linear models (regr.lm). A description of the learners can be found in the mlr3 reference web page.\nRepeat the example with a subset of predictive variables, identified by feature importance selection.\nDo a recherche about feature importance methods and apply it to a neural network learner."
  },
  {
    "objectID": "ml-intro.html#outlook",
    "href": "ml-intro.html#outlook",
    "title": "Basic Machine Learning with R with Toy Examples from Aquatic Ecology",
    "section": "5 Outlook",
    "text": "5 Outlook\nThe examples above should give a short introduction, so that the big world of ML techniques can be explored step by step. The upcoming mlr3 book (Kotthoff et al., 2023) covers (or will cover) additional topics like classification problems, resampling and benchmarking, hyperparameter optimization and model interpretation."
  }
]